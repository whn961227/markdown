## 电影推荐系统

### 项目系统架构

<img src="https://raw.githubusercontent.com/whn961227/images/master/data/20200825144259.png" style="zoom:80%;" />

【数据存储部分】

**业务数据库：**项目采用广泛应用的文档数据库 MongDB 作为主数据库，主要负责平台业务逻辑数据的存储。

【离线推荐部分】

**离线统计服务：**批处理统计性业务采用 Spark Core + Spark SQL 进行实现，实现对指标类数据的统计任务。

**离线推荐服务：**离线推荐业务采用 Spark Core + Spark MLlib 进行实现，采用 ALS 算法进行实现

【实时推荐部分】

**日志采集服务：**通过利用 Flume-ng 对业务平台中用户对于电影的一次评分行为 进行采集，实时发送到 Kafka 集群。

**消息缓冲服务：**项目采用 Kafka 作为流式数据的缓存组件，接受来自 Flume 的 数据采集请求。并将数据推送到项目的实时推荐系统部分。

**实时推荐服务：**项目采用 Spark Streaming 作为实时推荐系统，通过接收 Kafka 中缓存的数据，通过设计的推荐算法实现对实时推荐的数据处理，并将结构合并更 新到 MongoDB 数据库。

### 项目数据流程

<img src="https://raw.githubusercontent.com/whn961227/images/master/data/20200825144842.png"  />

【系统初始化部分】（数据加载模块）

0. 通过 Spark SQL 将系统初始化数据加载到 MongoDB 中

【离线推荐部分】

1. 离线统计服务从 MongoDB 中加载数据，将【电影平均评分统计】、【电影评分个数 统计】、【最近电影评分个数统计】三个统计算法进行运行实现，并将计算结果回写到 MongoDB 中；离线推荐服务从 MongoDB 中加载数据，通过 ALS 算法分别将【用户推荐结果矩阵】、【影片相似度矩阵】回写到 MongoDB 中。

【实时推荐部分】

2. Flume 从综合业务服务的运行日志中读取日志更新，并将更新的日志实时推送到 Kafka 中；Kafka 在收到这些日志之后，通过 kafkaStream 程序对获取的日志信息进行 过滤处理，获取用户评分数据流【UID|MID|SCORE|TIMESTAMP】，并发送到另外一个 Kafka 队列；Spark Streaming 监听 Kafka 队列，实时获取 Kafka 过滤出来的用户评分数据流，融合存储在 Redis 中的用户最近评分队列数据，提交给实时推荐算法， 完成对用户新的推荐结果计算；计算完成之后，将新的推荐结构和 MongDB 数据库中的推荐结果进行合并。

### 数据加载模块

```scala
// 1. 定义样例类
case class Movie(mid: Int, name: String, descri: String, timelong: String, issue: String, shoot: String, language: String,
                 genres: String, director: String, actors: String)
case class Rating(uid: Int, mid: Int, score: Double, timestamp: Int)
case class Tag(uid: Int, mid: Int, tag: String, timestamp: Int)
// 2. 把 mongo 配置封装成样例类
case class MongoConfig(uri: String, db: String)

object DataLoader {
    
	// 定义常量
    val MOVIE_DATA_PATH = "D:\\ideaProjects\\MovieRecommendSystem\\recommeder\\DataLoader\\src\\main\\resources\\movies.csv"
    val RATING_DATA_PATH = "D:\\ideaProjects\\MovieRecommendSystem\\recommeder\\DataLoader\\src\\main\\resources\\ratings.csv"
    val TAG_DATA_PATH = "D:\\ideaProjects\\MovieRecommendSystem\\recommeder\\DataLoader\\src\\main\\resources\\tags.csv"
    val MONGODB_MOVIE_COLLECTION = "Movie"
    val MONGODB_RATING_COLLECTION = "Rating"
    val MONGODB_TAG_COLLECTION = "Tag"
    val ES_MOVIE_INDEX = "Movie"
    
    def main(args: Array[String]): Unit = {
        // 定义用到的配置参数
        val config = Map(
          "spark.cores" -> "local[*]",
          "mongo.uri" -> "mongodb://192.168.91.107:27017/recommender",
          "mongo.db" -> "recommender"
        )
        
        // 创建sparkConfig对象
       	val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("DataLoader")
        // 创建sparkSession对象
    	val spark: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate()
        
        import spark.implicits._
        
        // 加载数据
        val movieRDD = spark.sparkContext.textFile(MOVIE_DATA_PATH)
        val movieDF: DataFrame = movieRDD.map {
          item => {
            val splits: Array[String] = item.split("\\^")
            Movie(splits(0).toInt, splits(1).trim, splits(2).trim, splits(3).trim, splits(4).trim, splits(5).trim
              , splits(6).trim, splits(7).trim, splits(8).trim, splits(9).trim)
          }
        }.toDF()

        val ratingRDD = spark.sparkContext.textFile(RATING_DATA_PATH)
        val ratingDF: DataFrame = ratingRDD.map {
          item => {
            val splits: Array[String] = item.split(",")
            Rating(splits(0).toInt, splits(1).toInt, splits(2).toDouble, splits(3).toInt)
          }
        }.toDF()

        val tagRDD = spark.sparkContext.textFile(TAG_DATA_PATH)
        val tagDF: DataFrame = tagRDD.map {
          item => {
            val splits: Array[String] = item.split(",")
            Tag(splits(0).toInt, splits(1).toInt, splits(2).trim, splits(3).toInt)
          }
        }.toDF()
        
        implicit val mongoConfig = MongoConfig(config("mongo.uri"), config("mongo.db"))
        // 将数据保存到mongo
        storeDataToMongoDB(movieDF, ratingDF, tagDF)
    }
}

def storeDataToMongoDB(movieDF: DataFrame, ratingDF: DataFrame, tagDF: DataFrame)(implicit mongoConfig: MongoConfig) = {
    // 新建mongodb连接
    val mongoClient = MongoClient(MongoClientURI(mongoConfig.uri))

    // 如果mongodb中有对应的数据库，那么应该删除
    mongoClient(mongoConfig.db)(MONGODB_MOVIE_COLLECTION).dropCollection()
    mongoClient(mongoConfig.db)(MONGODB_RATING_COLLECTION).dropCollection()
    mongoClient(mongoConfig.db)(MONGODB_TAG_COLLECTION).dropCollection()

    // 将DF数据写入到mongodb
    movieDF.write
      .option("uri", mongoConfig.uri)
      .option("collection", MONGODB_MOVIE_COLLECTION)
      .mode("overwrite")
      .format("com.mongodb.spark.sql")
      .save()
    ratingDF.write
      .option("uri", mongoConfig.uri)
      .option("collection", MONGODB_RATING_COLLECTION)
      .mode("overwrite")
      .format("com.mongodb.spark.sql")
      .save()
    tagDF.write
      .option("uri", mongoConfig.uri)
      .option("collection", MONGODB_TAG_COLLECTION)
      .mode("overwrite")
      .format("com.mongodb.spark.sql")
      .save()
    // 对数据表建索引
    mongoClient(mongoConfig.db)(MONGODB_MOVIE_COLLECTION).createIndex(MongoDBObject("mid" -> 1))
    mongoClient(mongoConfig.db)(MONGODB_RATING_COLLECTION).createIndex(MongoDBObject("uid" -> 1))
    mongoClient(mongoConfig.db)(MONGODB_RATING_COLLECTION).createIndex(MongoDBObject("mid" -> 1))
    mongoClient(mongoConfig.db)(MONGODB_TAG_COLLECTION).createIndex(MongoDBObject("uid" -> 1))
    mongoClient(mongoConfig.db)(MONGODB_TAG_COLLECTION).createIndex(MongoDBObject("mid" -> 1))

    mongoClient.close()
}
```

### 离线推荐服务

#### 统计推荐服务

```scala
case class Movie(mid: Int, name: String, descri: String, timelong: String, issue: String, shoot: String, language: String,
                 genres: String, director: String, actors: String)

case class Rating(uid: Int, mid: Int, score: Double, timestamp: Int)

case class MongoConfig(uri: String, db: String)

// 基准推荐样例类
case class Recommendation(mid: Int, score: Double)
// 电影类别Top10样例类
case class GenresRecommendation(genres: String, recs: Seq[Recommendation])

object StatisticsRecommender {
    
    val MONGODB_MOVIE_COLLECTION = "Movie"
    val MONGODB_RATING_COLLECTION = "Rating"
    
    //统计的表的名称
    val RATE_MORE_MOVIES = "RateMoreMovies"
    val RATE_MORE_RECENTLY_MOVIES = "RateMoreRecentlyMovies"
    val AVERAGE_MOVIES = "AverageMovies"
    val GENRES_TOP_MOVIES = "GenresTopMovies"
    
    def main(args: Array[String]): Unit = {
        
        val config = Map(
          "spark.cores" -> "local[*]",
          "mongo.uri" -> "mongodb://192.168.91.107:27017/recommender",
          "mongo.db" -> "recommender"
        )
        val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("StatisticsRecommender")
        val spark: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate()
        import spark.implicits._
        
        implicit val mongoConfig = MongoConfig(config("mongo.uri"), config("mongo.db"))
        
        // 从mongo加载数据
        val ratingDF: DataFrame = spark.read
          .option("uri", mongoConfig.uri)
          .option("collection", MONGODB_RATING_COLLECTION)
          .format("com.mongodb.spark.sql")
          .load()
          .as[Rating]
          .toDF()
        val movieDF: DataFrame = spark.read
          .option("uri", mongoConfig.uri)
          .option("collection", MONGODB_MOVIE_COLLECTION)
          .format("com.mongodb.spark.sql")
          .load()
          .as[Movie]
          .toDF()
        
        // 创建临时视图
    	ratingDF.createOrReplaceTempView("ratings")
        
        // TODO：不同的统计推荐结果
    	// 1. 历史评分数据最多
        val rateMoreMoviesDF: DataFrame = spark.sql("select mid, count(mid) as count from ratings group by mid")
        // 结果写入mongo
    	storeDFInMongoDB(rateMoreMoviesDF, RATE_MORE_MOVIES)
        
        // 2. 按照"yyyyMM"选取评分数据，统计个数
    	// 创建日期格式化工具
        val simpleDateFormat = new SimpleDateFormat("yyyyMM")
        // 注册UDF，转换时间格式
    	spark.udf.register("changeDate", (x: Int) => simpleDateFormat.format(new Date(x * 1000L)).toInt)
        // 对原始数据预处理，去掉 uid
        val ratingOfYearMonth: DataFrame = spark.sql("select mid, score, changeDate(timestamp) as yearmonth from ratings")
    	ratingOfYearMonth.createOrReplaceTempView("ratingOfMonth")
        // 统计按月分类的评分统计
    	val rateMoreRecentlyMoviesDF: DataFrame = spark.sql("select mid, count(mid) as count, yearmonth from ratingOfMonth group by yearmonth, mid order by yearmonth desc, count desc")
        // 写入mongo
        storeDFInMongoDB(rateMoreRecentlyMoviesDF, RATE_MORE_RECENTLY_MOVIES)
        
        // 3. 统计平均评分
        val avgMoviesDF: DataFrame = spark.sql("select mid, avg(score) as avg from ratings group by mid")
    	storeDFInMongoDB(avgMoviesDF, AVERAGE_MOVIES)
        
        // 4. 各类别电影top10
        // 定义类别
        val genres =
        List("Action", "Adventure", "Animation", "Comedy", "Crime", "Documentary", "Drama", "Famiy", "Fantasy", "Foreign"
          , "History", "Horror", "Music", "Mystery", "Romance", "Science", "Tv", "Thriller", "War", "Western")
        // 把平均分加入到movie表里，加一列
    	val movieWithScoreDF: DataFrame = movieDF.join(avgMoviesDF, "mid")
        // genres转成RDD做笛卡尔积
        val genresRDD: RDD[String] = spark.sparkContext.makeRDD(genres)
        val genresTopMovieDF: DataFrame = genresRDD.cartesian(movieWithScoreDF.rdd)
          .filter {
            // 筛选movie的genres包含当前类别
            case (genre, movieRow) => movieRow.getAs[String]("genres").toLowerCase.contains(genre.toLowerCase)
          }
          .map {
            case (genre, movieRow) => (genre, (movieRow.getAs[Int]("mid"), movieRow.getAs[Double]("avg")))
          }
          .groupByKey()
          .map {
            case (genre, items) =>
              GenresRecommendation(genre,
                items.toList.sortWith(_._2 > _._2).take(10).map(item => Recommendation(item._1, item._2)))
          }.toDF()
        // 写入mongo
        storeDFInMongoDB(genresTopMovieDF, GENRES_TOP_MOVIES)
        
        spark.stop()
    }
    
    def storeDFInMongoDB(DF: DataFrame, collectionName: String)(implicit mongoConfig: MongoConfig) = {
        DF.write
          .option("uri", mongoConfig.uri)
          .option("collection", collectionName)
          .mode("overwrite")
          .format("com.mongodb.spark.sql")
          .save()
      }
}
```

#### 基于隐语义模型的协同过滤推荐

```scala
case class MovingRating(uid: Int, mid: Int, score: Double, timestamp: Int)

case class MongoConfig(uri: String, db: String)

case class Recommendation(mid: Int, score: Double)

case class UserRecs(uid: Int, recs: Seq[Recommendation])

case class MovieRecs(mid: Int, recs: Seq[Recommendation])

object OfflineRecommender {
    
    val MONGODB_RATING_COLLECTION = "Rating"
    
    // 推荐表的名称
    val USER_RECS = "UserRecs"
    val MOVIE_RECS = "MovieRecs"
    val USER_MAX_RECOMMENDATION = 20
    
    def main(args: Array[String]): Unit = {
        val config = Map(
          "spark.cores" -> "local[*]",
          "mongo.uri" -> "mongodb://192.168.91.107:27017/recommender",
          "mongo.db" -> "recommender"
        )
        
        val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("OfflineRecommender")
        val spark: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate()
        import spark.implicits._

        implicit val mongoConfig = MongoConfig(config("mongo.uri"), config("mongo.db"))
        
        val ratingRDD: RDD[(Int, Int, Double)] = spark.read
          .option("uri", mongoConfig.uri)
          .option("collection", MONGODB_RATING_COLLECTION)
          .format("com.mongodb.spark.sql")
          .load()
          .as[MovingRating]
          .rdd
          .map(rating => (rating.uid, rating.mid, rating.score)) // 转换成 RDD，并且去掉时间戳
          .cache()
        
        // 获取rating数据中所有的uid，mid
        val userRDD: RDD[Int] = ratingRDD.map(_._1).distinct()
        val movieRDD: RDD[Int] = ratingRDD.map(_._2).distinct()
        
        // 训练隐语义模型
        val trainData: RDD[Rating] = ratingRDD.map(x => Rating(x._1, x._2, x._3))
        val (rank, iterations, lambda) = (200, 5, 0.1)
        val model: MatrixFactorizationModel = ALS.train(trainData, rank, iterations, lambda)
        
        // 基于用户和电影的隐特征，计算预测评分，得到用户的推荐列表
        // 求用户和电影的笛卡尔积，得到空矩阵
        val userMoviesRDD: RDD[(Int, Int)] = userRDD.cartesian(movieRDD)
        // 调用model预测评分
        val preRatingsRDD: RDD[Rating] = model.predict(userMoviesRDD)
        // 过滤评分大于0
        val userRecsDF: DataFrame = preRatingsRDD.filter(_.rating > 0)
          .map(rating => (rating.user, (rating.product, rating.rating))) // （uid,(mid,score)）
          .groupByKey()
          .map {
            case (uid, recs) => UserRecs(uid, recs.toList.sortWith(_._2 > _._2).take(USER_MAX_RECOMMENDATION)
              .map(x => Recommendation(x._1, x._2)))
          }
          .toDF()
        // 结果写入mongo
        userRecsDF.write
          .option("uri", mongoConfig.uri)
          .option("collection", USER_RECS)
          .mode("overwrite")
          .format("com.mongodb.spark.sql")
          .save()
        
        // 基于电影的隐特征，计算相似度矩阵，得到电影的相似度列表
        val movieFeaturesRDD: RDD[(Int, DoubleMatrix)] = model.productFeatures.map {
          case (mid, features) => (mid, new DoubleMatrix(features))
        }
        // 对所有电影计算笛卡尔积
        val movieRecs: DataFrame = movieFeaturesRDD.cartesian(movieFeaturesRDD)
          .filter {
            case (a, b) => a._1 != b._1
          }
          .map {
            case (a, b) => {
              val simScore = this.consinSim(a._2, b._2)
              (a._1, (b._1, simScore))
            }
          }
          .filter(_._2._2 > 0.6)
          .groupByKey()
          .map {
            case (mid, recs) => MovieRecs(mid, recs.toList.sortWith(_._2 > _._2).map(x => Recommendation(x._1, x._2)))
          }.toDF()
        // 结果写入mongo
        movieRecs.write
          .option("uri", mongoConfig.uri)
          .option("collection", MOVIE_RECS)
          .mode("overwrite")
          .format("com.mongodb.spark.sql")
          .save()
        
        spark.stop()
    }
}
```

### 实时推荐服务

```scala
case class MongoConfig(uri: String, db: String)

case class UserRecs(uid: Int, recs: Seq[Recommendation])

case class Recommendation(mid: Int, score: Double)

case class MovieRecs(mid: Int, recs: Seq[Recommendation])

object ConnHelper extends Serializable {
    lazy val jedis = new Jedis("192.168.91.107")
    lazy val mongoClient = MongoClient(MongoClientURI("mongodb://192.168.91.107:27017/recommender"))
}

object StreamingRecommender {
    
    val MAX_USER_RATINGS_NUM = 20
    val MAX_SIM_MOVIES_NUM = 20
    val MONGODB_STREAM_RECS_COLLECTION = "StreamRecs"
    val MONGODB_RATING_COLLECTION = "Rating"
    val MONGODB_MOVIE_RECS_COLLECTION = "MovieRecs"
    
    def main(args: Array[String]): Unit = {
        val config = Map(
            "spark.cores" -> "local[*]",
            "mongo.uri" -> "mongodb://192.168.91.107:27017/recommender",
            "mongo.db" -> "recommender",
            "kafka.topic" -> "recommender"
        )

        val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("StreamingRecommender")
        val spark: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate()

        val sc: SparkContext = spark.sparkContext
        val ssc = new StreamingContext(sc, Seconds(2))
        import spark.implicits._

        implicit val mongoConfig = MongoConfig(config("mongo.uri"), config("mongo.db"))
        
        // 加载MovieRecs数据，作为广播变量
        val simMovieMatrix: collection.Map[Int, Map[Int, Double]] = spark.read
        .option("uri", mongoConfig.uri)
        .option("collection", MONGODB_MOVIE_RECS_COLLECTION)
        .format("com.mongodb.spark.sql")
        .load()
        .as[MovieRecs]
        .rdd
        .map {
            item => (item.mid, item.recs.map(x => (x.mid, x.score)).toMap)
        }
        .collectAsMap()
        val simMovieMatrixBroadCast: Broadcast[collection.Map[Int, Map[Int, Double]]] = sc.broadcast(simMovieMatrix)
        
        // 通过kafka创建一个DStream
        val kafkaDStream: ReceiverInputDStream[(String, String)] = KafkaUtils.createStream(ssc
          , "hadoop7:2181"
          , "recommender"
          , Map("recommender" -> 3))
        
        // UID|MID|SCORE|TIMESTAMP
        // 产生评分流
        val ratingStream: DStream[(Int, Int, Double, Int)] = kafkaDStream.map {
          case msg =>
            val attr: Array[String] = msg._2.split("\\|")
            (attr(0).toInt, attr(1).toInt, attr(2).toDouble, attr(3).toInt)
        }
        
        // 核心算法
        ratingStream.foreachRDD(rdd => rdd.map {
          case (uid, mid, score, timestamp) =>
            println(">>>>>>>>>>>>>>>>>>>>>>data received" + (uid, mid, score, timestamp))

            // 1. 从redis中获取最近的K次评分，保存成Array[(mid, score)]
            val userRecentlyRatings = getUserRecentlyRating(MAX_USER_RATINGS_NUM, uid, ConnHelper.jedis)
            // 2. 从相似度矩阵中取出当前电影最相似的N个电影作为备选列表，Array[mid]
            val candidateMovies = getTopSimMovies(MAX_SIM_MOVIES_NUM, mid, uid, simMovieMatrixBroadCast.value)
            // 3. 对每个备选电影计算推荐优先级，得到当前用户的实时推荐列表 Array[(mid, score)]
            val streamRecs = computeMovieScores(candidateMovies, userRecentlyRatings, simMovieMatrixBroadCast.value)
            // 4. 把推荐数据保存到mongo
            saveDataToMongoDB(uid, streamRecs)

          }.count()
    	)
        
        ssc.start()

        println(">>>>>>>>>>>>>>>>>>>>>>>>>>>>>streaming started")
        ssc.awaitTermination()
    }
    
    def getUserRecentlyRating(num: Int, uid: Int, jedis: Jedis): Array[(Int, Double)] = {
        // 从redis中取出用户的num个评分      uid:1  -> mid:score
        jedis.lrange("uid:" + uid.toString, 0, num).map {   // mid:score
            item =>
            val attr: Array[String] = item.split("\\:")
            (attr(0).trim.toInt, attr(1).trim.toDouble)
        }.toArray
    }
    
    def getTopSimMovies(num: Int, mid: Int, uid: Int, simMovies: collection.Map[Int, Map[Int, Double]])
    (implicit mon goConfig: MongoConfig): Array[Int] = {
        // 1. 从相似度矩阵中取出所有相似的电影
        val allSimMovies: Array[(Int, Double)] = simMovies(mid).toArray
        // 2. 从mongo中取得所有用户看过的电影
        val ratingExist: Array[Int] = ConnHelper.mongoClient(mongoConfig.db)(MONGODB_RATING_COLLECTION)
        .find(MongoDBObject("uid" -> uid)).toArray
        .map {
            item => item.get("mid").toString.toInt
        }
        // 3. 把看过的电影过滤
        allSimMovies.filter(x => !ratingExist.contains(x._1))
        .sortWith(_._2 > _._2)
        .take(num)
        .map(x => x._1)

    }
    
    // 获取两个电影的相似度
    def getMoviesSimScore(mid1: Int, mid2: Int, simMovies: collection.Map[Int, Map[Int, Double]]): Double = {
        simMovies.get(mid1) match {
            case Some(sims) =>
            sims.get(mid2) match {
                case Some(score) => score
                case None => 0.0
            }
            case None => 0.0
        }
    }
    
    // 求log，底数默认10
    def log(m: Int): Double = {
        val N = 10
        math.log(m) / math.log(N)
    }
    
    def computeMovieScores(candidateMovies: Array[Int], userRecentlyRatings: Array[(Int, Double)]
                           , simMovies: collection.Map[Int, Map[Int, Double]]): Array[(Int, Double)] = {
        // 用于保存备选电影的基础得分
        val scores: ArrayBuffer[(Int, Double)] = ArrayBuffer[(Int, Double)]()
        // 保存每一个备选电影的增强/减弱因子
        val increMap: mutable.HashMap[Int, Int] = mutable.HashMap[Int, Int]()
        val decreMap: mutable.HashMap[Int, Int] = mutable.HashMap[Int, Int]()

        for (candidateMovie <- candidateMovies; userRecentlyRating <- userRecentlyRatings) {
            val simScore = getMoviesSimScore(candidateMovie, userRecentlyRating._1, simMovies)

            if (simScore > 0.7) {
                // 计算基础推荐得分  ((mid, 相似度 * 评分))
                scores += ((candidateMovie, simScore * userRecentlyRating._2))
                if (userRecentlyRating._2 > 3) { // （mid  ->   高分电影数）
                    increMap(candidateMovie) = increMap.getOrDefault(candidateMovie, 0) + 1
                } else {    // (mid   ->   低分电影数)
                    decreMap(candidateMovie) = decreMap.getOrDefault(candidateMovie, 0) + 1
                }
            }
        }
        // 根据备选电影的mid做groupby，根据公式求最后的推荐评分
        scores.groupBy(_._1).map {
            case (mid, scoreList) =>
            (mid, scoreList.map(_._2).sum / scoreList.length
             + log(increMap.getOrDefault(mid, 1)) - log(decreMap.getOrDefault(mid, 1)))
        }.toArray
    }
    
    def saveDataToMongoDB(uid: Int, streamRecs: Array[(Int, Double)])(implicit mongoConfig: MongoConfig) = {
        // 定义到表的连接
        val streamRecsCollection: MongoCollection = ConnHelper.mongoClient(mongoConfig.db)(MONGODB_STREAM_RECS_COLLECTION)
        // 如果已经有uid对应的数据，删除
        streamRecsCollection.findAndRemove(MongoDBObject("uid" -> uid))
        // 将推荐列表数据存入表中
        streamRecsCollection.insert(MongoDBObject("uid" -> uid, "recs" -> streamRecs.map{
            x => MongoDBObject("mid" -> x._1, "score" -> x._2)
        }))
    }
}
```

### 实时系统联调

我们的系统实时推荐的数据流向是：业务系统 -> 日志 -> flume 日志采集 -> kafka streaming 数据清洗和预处理 -> spark streaming 流式计算。

#### 配置并启动 Flume

在 flume 的 conf 目录下新建 log-kafka.properties，对 flume 连接 kafka 做配置：

```xml
agent.sources = exectail
agent.channels = memoryChannel
agent.sinks = kafkasink

agent.sources.exectail.type = exec
agent.sources.exectail.command = tail -f /mnt/d/Projects/BigData/MovieRecommenderSystem/businessServer/src/main/log/agent.log
agent.sources.exectail.interceptors=i1
agent.sources.exectail.interceptors.i1.type=regex_filter
# 定义日志过滤前缀的正则
agent.sources.exectail.interceptors.i1.regex=.+MOVIE_RATING_PREFIX.+

agent.sources.exectail.channels = memoryChannel

agent.sinks.kafkasink.type = org.apache.flume.sink.kafka.KafkaSink
agent.sinks.kafkasink.kafka.topic = log
agent.sinks.kafkasink.kafka.bootstrap.servers = localhost:9092
agent.sinks.kafkasink.kafka.producer.acks = 1
agent.sinks.kafkasink.kafka.flumeBatchSize = 20

agent.sinks.kafkasink.channel = memoryChannel

agent.channels.memoryChannel.type = memory

agent.channels.memoryChannel.capacity = 10000
```



#### 构建 Kafka Streaming 程序

```java
public class Application {
    public static void main(String[] args) {

        String brokers = "hadoop7:9092";
        String zookeepers = "hadoop7:2181";

        // 定义输入和输出的 topic
        String from = "log";
        String to = "recommender";

        // 定义 kafka streaming 的配置
        Properties settings = new Properties();
        settings.put(StreamsConfig.APPLICATION_ID_CONFIG, "logFilter");
        settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, brokers);
        settings.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeepers);

        StreamsConfig config = new StreamsConfig(settings);

        // 拓扑建构器
        TopologyBuilder builder = new TopologyBuilder();
        // 定义流处理的拓扑结构
        builder.addSource("SOURCE", from)
                .addProcessor("PROCESS", () -> new LogProcessor(), "SOURCE")
                .addSink("SINK", to, "PROCESS");

        KafkaStreams streams = new KafkaStreams(builder, config);
        streams.start();

        System.out.println("kafka stream started >>>>>>>>>>>>>>>");

    }
}

public class LogProcessor implements Processor<byte[], byte[]> {

    private ProcessorContext context;

    @Override
    public void init(ProcessorContext processorContext) {
        this.context = processorContext;
    }

    @Override
    public void process(byte[] key, byte[] value) {
        String input = new String(value);
        // 根据前缀过滤日志，提取信息
        if(input.contains("MOVIE_RATING_PREFIX:")){
            System.out.println("movie rating data coming!!!" + input);
            input = input.split("MOVIE_RATING_PREFIX:")[1].trim();
            context.forward("logProcessor".getBytes(), input.getBytes());
        }
    }

    @Override
    public void punctuate(long l) {

    }

    @Override
    public void close() {

    }
}
```

