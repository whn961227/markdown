

## 用户行为数仓采集

### 数据仓库概念

数据仓库是**为企业所有决策制定过程，提供所有系统数据支持的战略集合。**

通过对数据仓库中数据的分析，可以帮助企业，**改进业务流程，控制成本，提高产品质量**等

数据仓库，并不是数据的最终目的地，而是为数据最终的目的地做好准备。这些准备包括对数据的：**清洗，转义，分类，重组，合并，拆分，统计**等等

<img src="https://raw.githubusercontent.com/whn961227/images/master/data/20200825221441.png" alt="image-20200825221441648" style="zoom: 80%;" />

### 项目需求及架构设计

#### 系统数据流程设计

![image-20200914153656334](https://raw.githubusercontent.com/whn961227/images/master/data/20200914153656.png)

#### 数仓分层架构表

![](https://raw.githubusercontent.com/whn961227/images/master/data/20200914161238.png)

![image-20200914162418820](https://raw.githubusercontent.com/whn961227/images/master/data/20200914162418.png)

### 数据生成模块

#### 数据格式样例

**事件日志数据**

![](https://raw.githubusercontent.com/whn961227/images/master/data/20200805214153.png)

**启动日志数据**

<img src="https://raw.githubusercontent.com/whn961227/images/master/data/20200805214208.png"  />



### 数据采集模块

#### Hadoop 安装

#### Zookeeper 安装

#### 日志生成

#### 采集日志 Flume

##### 日志采集 Flume 安装

##### 项目经验之 Flume 组件

###### Source

* Taildir Source 相比 Exec Source、Spooling Directory Source 的优势

  **Taildir Source：**断点续传、多目录。Flume 1.6 以前需要自己自定义 Source 记录每次读取文件位置，实现断点续传

  **Exec Source：**可以实时搜集数据，但是在 Flume 不运行或者 Shell 命令出错的情况下，数据将会丢失

  **Spooling Directory Source：**监控目录，不支持断点续传

* batchSize 大小如何设置

  Event 1K 左右，500-1000 合适（默认为 100）

###### Channel

采用 **Kafka Channel**，省去了 Sink，提高了效率

#####  日志采集 Flume 配置

##### Flume 的 ETL 和分类型拦截器

```java
/*
ETL 拦截器：LogETLInterceptor 实现 Interceptor 接口
    ETL：根据 start 标签区分 启动日志（JSON） 和 事件日志（时间戳|JSON）
    校验启动日志：1. 是否为空
                2. 是否符合 JSON 格式 ====> 以 '{' 开头且以 '}' 结尾
    校验事件日志：1. 是否为空
                2. 按 '|' 切割 ====> 判断长度是否为 2
                3. 判断时间戳格式 ====> 是否长度为 13 且 是否每一位都是数字
                4. 判断 JSON 格式 ====> 以 '{' 开头且以 '}' 结尾
    静态内部类 Builder 实现 Interceptor.Builder 接口
    	作用：用来实例化 LogETLInterceptor
*/
/*
日志类型区分拦截器：LogTypeInterceptor 实现 Interceptor 接口
	将启动日志和事件日志区分开来，方便发往 Kafka 的不同 Topic
	向启动日志的 event 的 headers 中添加 "topic","topic_start"
	向事件日志的 event 的 headers 中添加 "topic","topic_event"
	静态内部类 Builder 实现 Interceptor.Builder 接口
    	作用：用来实例化 LogTypeInterceptor
*/
/*
如何实现一个拦截器
1. 定义拦截器类实现 Interceptor 接口
2. 重写 4 个方法：
	（1）initialize 初始化
	（2）public Event intercept(Event event) 单 Event 处理
	（3）public List<Event> intercept(List<Event> events) 多 Event 处理
	（4）close()
3. 创建一个静态内部类 Builder 实现 Interceptor.Builder 接口
*/
```

> event 包含 header 和 body：body 才是我们实际使用中真正传输的数据，header 传输的数据，我们是不会 Sink 出去的
>
> header 的话，就是在分装 event 对象的时候，我们可以自定义的设置一些 key-value 对，这样做的目的，是为了后续的 **通道多路复用** 做准备的
>
> 在 Source 端产出 event 的时候，通过 header 去区别对待不同的 event，然后在 sink 端的时候，可以通过 header 中的 key 来将不同的 event 放入不同的 Channel 中，紧接着，再通过配置多个 Sink 去不同的 Channel 中取出 event，这样就将 event 分流出去了
>
> 但是这里有一个前提：**不建议通过对 event 的 body 解析来设置 header，因为 flume 就是一个水槽，水槽是不会在中间对水进行加工的，要加工，等水流出去了再加工**

##### 日志采集 Flume 启动停止脚本



#### Kafka 安装

##### Kafka  集群安装

##### Kafka 集群启动停止脚本

##### Kafka Manager 安装

##### Kafka Manager 启动停止脚本

##### 项目经验之 Kafka 压力测试

##### 项目经验之 Kafka 机器数量计算



#### 消费 Kafka 数据 Flume

##### 日志消费 Flume 配置

##### 日志消费 Flume 启动停止脚本

##### 项目经验之 Flume 内存优化

##### 项目经验之 Flume 组件



### 总结

#### 数仓概念总结

1. 数仓的输入数据源和输出系统分别是什么

   输入系统：埋点产生的用户行为数据、Java EE 后台产生的业务数据

   输出系统：报表系统、用户画像系统、推荐系统

#### 项目需求及架构总结

#### 数据采集模块总结

##### Linux & Shell 相关总结

##### Hadoop 相关总结

##### Zookeeper 相关总结

##### Flume 相关总结

##### Kafka 总结

