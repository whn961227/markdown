## 用户行为数据仓库

### 数仓分层概念

#### 数据仓库分层

<img src="https://raw.githubusercontent.com/whn961227/images/master/data/20200808143114.png"  />

ODS：原始数据层，**存放原始数据**，直接加载原始日志、数据，数据保持原貌不做处理

DWD：结构和粒度与原始表保持一致，**对 ODS 层数据进行清洗（去除空值、脏数据，超出极限范围的数据，行式存储改为列存储，改压缩格式）**

DWS：以 DWD 为基础，**进行轻度汇总**

ADS：**为各种统计报表提供数据**

#### 数据仓库为什么要分层

1. 把复杂问题简单化

   将一个复杂的**任务分解**成多个步骤来完成，每一层只处理单一的步骤，**比较简单**，并且**方便定位问题**

2. 减少重复开发

   规范数据分层，通过的**中间层数据**，能够减少极大的重复计算，**增加**一次计算结果的**复用性**

3. 隔离原始数据

   不论是数据的异常还是数据的敏感性，使真实数据与统计数据**解耦**开

### 数仓搭建环境准备

#### Hive & MySQL 安装

#### Hive 运行引擎 Tez

#### 项目经验之元数据备份

### 数仓搭建之 ODS 层

#### 创建数据库

#### ODS 层

##### 创建启动日志表 ods_start_log

```sql
-- 如果要创建的表已经存在，先删除该表
drop table if exists ods_start_log;
-- 创建一张外部表，字段就是一个 String 类型的 Json
create external table ods_start_log(line string)
-- 该表按照日期分区
partitioned by (`dt` string)
-- LZO 压缩格式处理
stored as
 INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
 OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-- 设置数据存储格式
location '/warehouse/gmall/ods/ods_start_log';
```

```sql
-- 加载数据
load data inpath '/origin_data/gmall/log/topic_start/2019-02-10' 
into table gmall.ods_start_log partition (dt='2019-02-10');
```

##### 创建事件日志表 ods_event_log

```sql
-- 如果要创建的表已经存在，先删除该表
drop table if exists ods_event_log;
-- 创建一张外部表，字段就是一个 String 类型的 Json
create external table ods_event_log(line string)
-- 该表按照日期分区
partitioned by (`dt` string)
-- LZO 压缩格式处理
stored as
 INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
 OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-- 设置数据存储格式
location '/warehouse/gmall/ods/ods_event_log';
```

```sql
-- 加载数据
load data inpath '/origin_data/gmall/log/topic_event/2019-02-10' 
into table gmall.ods_event_log partition (dt='2019-02-10');
```

##### Shell 中单引号和双引号区别

##### ODS 层加载数据脚本

### 数仓搭建之 DWD 层

#### DWD 层启动表数据解析

##### 创建启动表

##### 向启动表汇导入数据

##### DWD 层启动表加载数据脚本

#### DWD 层事件表数据解析

##### 创建基础明细表

##### 自定义 UDF 函数（解析公共字段）

![](https://raw.githubusercontent.com/whn961227/images/master/data/20200809155651.png)

```java
/*
BaseFieldUDF 继承 UDF
重写 evaluate(String line, String jsonkeysString) 函数
1. 获取所有的 key
2. line		服务器时间|Json
3. 校验
4. 对 logContents[1] 创建 Json 对象
5. 获取公共字段的 Json 对象
6. 循环遍历
7. 拼接事件字段和服务器时间字段
*/
```

##### 自定义 UDTF 函数（解析具体事件字段）

![](https://raw.githubusercontent.com/whn961227/images/master/data/20200809162820.png)

```java
/*
EventJsonUDTF 继承 GenericUDTF
重写 initialize() process() close() 方法
initialize()：指定输出参数的名称和类型
	"event_name"：PrimitiveObjectInspectorFactory.javaStringObjectInspector
	"event_json"：PrimitiveObjectInspectorFactory.javaStringObjectInspector
process()：输入 1 条记录，输出若干条结果
	1. 获取传入的 et
	2. 如果传进来的数据为空，直接返回过滤掉该数据
	3. 获取一共有几个事件 用 JSONArray 保存
	4. 循环遍历每一个事件
	5. 取出每个事件名称
	6. 取出每个事件整体
	7. 将结果返回
close()：当没有记录处理的时候该方法会被调用，用来清理代码或者产生额外的输出
*/
```

**打包上传**

**将 jar 包添加到 Hive 的 ClassPath**

```sql
hive (gmall)> add jar /opt/module/hive/hivefunction-1.0-SNAPSHOT.jar;
```

**创建临时函数与开发好的 Java class 关联**

```sql
hive (gmall)> 
create temporary function base_analizer as 'com.atguigu.udf.BaseFieldUDF';
create temporary function flat_analizer as 'com.atguigu.udtf.EventJsonUDTF';
```

##### 解析事件日志基础明细表

##### DWD 层数据解析脚本

#### DWD 层事件表获取

### 业务知识准备

### 需求一：用户活跃主题

#### DWS 层

##### 每日活跃设备明细

##### 每周活跃设备明细

##### 每月活跃设备明细

##### DWS 层加载数据脚本

#### ADS 层

##### 活跃设备数

##### ADS 层加载数据脚本

### 需求二：用户新增主题

##### DWS 层（每日新增设备明细表）

##### ADS 层（每日新增设备表）

### 需求三：用户留存主题

